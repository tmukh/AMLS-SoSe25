{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d85321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Task 1.3 ECG Classification - Data Augmentation\n",
    "# ==============================================\n",
    "\n",
    "# ✅ Cell 1: Imports and Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31695e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4943, Val: 1236, Test: 2649\n"
     ]
    }
   ],
   "source": [
    "# ✅ Cell 2: Load Data Splits and Raw Data\n",
    "def load_bin_file(filepath):\n",
    "    signals = []\n",
    "    with open(filepath, 'rb') as f:\n",
    "        while True:\n",
    "            length_bytes = f.read(4)\n",
    "            if not length_bytes:\n",
    "                break\n",
    "            length = struct.unpack('i', length_bytes)[0]\n",
    "            signal_bytes = f.read(length * 2)\n",
    "            signal = np.frombuffer(signal_bytes, dtype=np.int16)\n",
    "            signals.append(signal)\n",
    "    return signals\n",
    "\n",
    "X_train = load_bin_file('../1.1_dataset/data/X_train.bin')\n",
    "X_test = load_bin_file('../1.1_dataset/data/X_test.bin')\n",
    "\n",
    "train_indices = np.load('../1.1_dataset/data/train_indices.npy')\n",
    "val_indices = np.load('../1.1_dataset/data/val_indices.npy')\n",
    "y_train_split = np.load('../1.1_dataset/data/y_train_split.npy')\n",
    "y_val_split = np.load('../1.1_dataset/data/y_val_split.npy')\n",
    "\n",
    "X_train_split = [X_train[i] for i in train_indices]\n",
    "X_val_split = [X_train[i] for i in val_indices]\n",
    "\n",
    "print(f\"Train: {len(X_train_split)}, Val: {len(X_val_split)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a817626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Cell 3: Simple Data Augmentation Functions\n",
    "def amplitude_scaling(signal, scale_range=(0.9, 1.1)):\n",
    "    return signal * np.random.uniform(*scale_range)\n",
    "\n",
    "def add_noise(signal, noise_std=5.0):\n",
    "    noise = np.random.normal(0, noise_std, size=signal.shape)\n",
    "    return signal + noise\n",
    "\n",
    "def time_shift(signal, max_shift=200):\n",
    "    shift = np.random.randint(-max_shift, max_shift)\n",
    "    return np.roll(signal, shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3389f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import resample\n",
    "\n",
    "def time_stretch(signal, stretch_range=(0.8, 1.2)):\n",
    "    \"\"\"Time stretch signal by resampling.\"\"\"\n",
    "    factor = np.random.uniform(*stretch_range)\n",
    "    new_length = int(len(signal) * factor)\n",
    "    stretched = resample(signal, new_length)\n",
    "    # Pad or crop to original length\n",
    "    if new_length > len(signal):\n",
    "        return stretched[:len(signal)]\n",
    "    else:\n",
    "        return np.pad(stretched, (0, len(signal) - new_length), mode='constant')\n",
    "\n",
    "def frequency_dropout(signal, drop_prob=0.1):\n",
    "    \"\"\"Randomly zero out some frequencies in the Fourier domain.\"\"\"\n",
    "    fft = np.fft.fft(signal)\n",
    "    num_bins = len(fft)\n",
    "    mask = np.random.rand(num_bins) > drop_prob\n",
    "    fft *= mask\n",
    "    return np.fft.ifft(fft).real\n",
    "\n",
    "def random_crop(signal, crop_size_ratio=0.9):\n",
    "    \"\"\"Randomly crop a portion and resize back to original length.\"\"\"\n",
    "    crop_size = int(len(signal) * crop_size_ratio)\n",
    "    start = np.random.randint(0, len(signal) - crop_size)\n",
    "    cropped = signal[start:start+crop_size]\n",
    "    return resample(cropped, len(signal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dcbbe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedECGDataset(Dataset):\n",
    "    def __init__(self, signals, labels=None, augment=False):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.signals[idx].astype(np.float32)\n",
    "        if self.augment:\n",
    "            signal = amplitude_scaling(signal)\n",
    "            # signal = add_gaussian_noise(signal)\n",
    "            signal = time_shift(signal)\n",
    "            signal = time_stretch(signal)\n",
    "            signal = frequency_dropout(signal)\n",
    "            signal = random_crop(signal)\n",
    "        signal = torch.tensor(signal, dtype=torch.float32)\n",
    "        length = len(signal)\n",
    "        if self.labels is not None:\n",
    "            return signal, length, self.labels[idx]\n",
    "        else:\n",
    "            return signal, length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Cell 5: Data Loaders\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn_train(batch):\n",
    "    signals, lengths, labels = zip(*batch)\n",
    "    signals = pad_sequence(signals, batch_first=True)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    labels = torch.tensor(labels)\n",
    "    return signals, lengths, labels\n",
    "\n",
    "def collate_fn_test(batch):\n",
    "    signals, lengths = zip(*batch)\n",
    "    signals = pad_sequence(signals, batch_first=True)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    return signals, lengths\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    AugmentedECGDataset(X_train_split, y_train_split, augment=True),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_train)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    AugmentedECGDataset(X_val_split, y_val_split, augment=False),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40e0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Cell 6: Load Model Class (from 1.2)\n",
    "class ECG_1D_CNN_Enhanced(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(1, 32, 7, padding=3), nn.BatchNorm1d(32), nn.ReLU(),\n",
    "                nn.MaxPool1d(2), nn.Dropout(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(32, 64, 11, padding=5), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "                nn.MaxPool1d(2), nn.Dropout(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(64, 128, 15, padding=7), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "                nn.MaxPool1d(2), nn.Dropout(0.3)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(128, 256, 21, padding=10), nn.BatchNorm1d(256), nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool1d(1), nn.Dropout(0.4)\n",
    "            )\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.5), nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        x = (x - x.mean(dim=1, keepdim=True)) / (x.std(dim=1, keepdim=True) + 1e-8)\n",
    "        x = x.unsqueeze(1)\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b858ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Cell 7: Training Loop\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_split), y=y_train_split)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce)\n",
    "        return (self.alpha * (1 - pt) ** self.gamma * ce).mean()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=EPOCHS):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    criterion = FocalLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', 0.5, 3)\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for signals, lengths, labels in train_loader:\n",
    "            signals, labels = signals.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(signals)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for signals, lengths, labels in val_loader:\n",
    "                signals, labels = signals.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(signals)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        scheduler.step(f1)\n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Val F1={f1:.4f}\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc28f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.5768, Val F1=0.1853\n",
      "Epoch 2: Loss=0.4719, Val F1=0.1930\n",
      "Epoch 3: Loss=0.4511, Val F1=0.2169\n",
      "Epoch 4: Loss=0.4328, Val F1=0.2751\n",
      "Epoch 5: Loss=0.4183, Val F1=0.2737\n",
      "Epoch 6: Loss=0.4072, Val F1=0.2860\n"
     ]
    }
   ],
   "source": [
    "# ✅ Cell 8: Train & Evaluate\n",
    "model = ECG_1D_CNN_Enhanced().to(DEVICE)\n",
    "model = train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b8fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Cell 9: Save Predictions on Test Set\n",
    "test_loader = DataLoader(\n",
    "    AugmentedECGDataset(X_test, augment=False),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda b: pad_sequence([s for s, _ in b], batch_first=True)\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for signals in test_loader:\n",
    "        signals = signals.to(DEVICE)\n",
    "        outputs = model(signals)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "pd.DataFrame({'id': np.arange(len(test_preds)), 'class': test_preds}).to_csv('augment.csv', index=False)\n",
    "print(\"✅ Predictions saved to 'augment.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab887b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
